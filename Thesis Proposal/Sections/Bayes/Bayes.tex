\subsection{Bayesian Statistics}

In this section, I introduce some key concepts of Bayesian statistics to be used in the remainder of the proposal.  I begin with explaining how Bayesianism differs from Frequentism in philosophy.  I then introduce Bayes Nets as a tool for writing down complex Bayesian models in such a way as to preserve economy of thought.  Finally, I discuss some finer points of Bayesian modelling, such as model diagnostics and MCMC computation.


\subsubsection{Bayesians v. Frequentists}

Statistical methods taught in most  university classes are Frequentist methods.  In Frequentism, probability is understood as the long term relative frequency of an event occurring.  Consequently, Statisticians assess estimators by considering the behaviour of the estimator under repeated construction.  

This is exemplified by the confidence interval, which is named so not because it has a 95\% probability of containing the true estimand\footnote{To the dismay of students learning about probability for the first time.}, but because the long term relative frequency of confidence intervals containing the true estimated is 95\%.  Thus, Frequentists never make probabilistic statements about any one confidence interval in particular, only about the behaviour of confidence intervals constructed ad infinitum.  Frequentistism is strongly contrasted against Bayesianism, where probability represents a strength in a belief \cite{gelman2013bayesian}.  Under the Bayesian paradigm, it is completely acceptable to make probabilistic statements about a particular interval.  In fact, all inferences made from a Bayesian data analysis are made in terms of probabilistic statements.


\subsubsection{Bayesian Networks}

Core to Bayesian statistics is Bayes' Theorem
%
\begin{equation}\label{Bayes}
	P( \bm{\theta} \vert \mathbf{x}) \propto P(\mathbf{x} \vert \bm{\theta}) P(\bm{\theta}) \>.
\end{equation}
%
Bayesian's refer to $  P( \bm{\theta} \vert \mathbf{x}) $, $ P(\mathbf{x} \vert \bm{\theta})  $, and $P(\bm{\theta})$ as the \textit{posterior}, \textit{the likelihood}, and \textit{the prior} respectively.  Since the product of Bayes' theorem is a probability distribution (i.e. the probability of the parameters conditioned on the data), inferences resulting from a Bayesian analysis are expressed in probabilistic statements. Bayesian modelling begins by specifying a full probability model for the phenomenon.  A likelihood for the data generating process is specified, and prior knowledge about the parameters is codified in terms of a probability distribution (i.e. the prior).  Conditioning on the observed data is performed, and the posterior is calculated and interpreted.  Finally, the resulting model is evaluated and the implications of the resulting posterior are assessed.

Bayesian models can become quite complex, so to ease economy of thought, Bayesian networks (Bayes nets) can be used to exposit the relationship amongst the various parameters and observed data.  A Bayes net is a directed acyclic graph which represents a factorization of the joint probability distribution of the model. The nodes of the graph denote random variables, while the edges denote dependence of the child node on the parent node \cite{Bishop2006pattern}. 

Shown in \cref{bayesnet} is an example of a Bayes net for Gelman's rat tumour example in \cite{gelman2013bayesian}, which I explain here.  A total of $ N = 71 $ experiments on lab rats have been conducted in the past to assess the risk of developing endometrial stromyl polyps.  A rat can either develop the endometrial stromal polyp, or not develop the endometrial stromyl polyp, so the number of rats which develop the polyp, $ y_i $, is modelled as binomial.  Each of the 71 previous experiments is modelled as having it's own risk of developing the polyp, $ \theta_i $, which we postulate are drawn from a beta distribution with parameters $ \alpha $ and $ \beta $.  

Traditionally, the model would be written as 
%
\begin{align*}
	[\alpha,\beta]^T &\sim P(\alpha, \beta) \\
	\theta_i &\sim \operatorname{Beta}(\alpha,\beta) \quad i = 1 \dots N\\
	y_i &\sim \operatorname{Binomial}(\theta_i ; n_i) \quad i = 1 \dots N
	\end{align*}
%
Here, $ P(\alpha, \beta)  $ is the prior for the parameters of the beta distribution, and $ n_i $ is the number of rats in the $ i^{th} $ experiment.  This same model is written as a Bayes net in \cref{bayesnet}.   The node containing $ \alpha $ and $ \beta $ is the parent of $ \theta $, indicating that $ \theta $ relies on $ \alpha $ and $ \beta $, and $ y $ is the child of $ \theta $, indicating that $ y $ relies on $ \theta $.  The rectangle surrounding $ \theta $ and $ y $ signifies that there are $ N $ such copies of these random variables.  Instead of explicitly writing out all $ N=71 $ of these random variables, we instead place them in what is known as a ``plate", and indicate in the bottom corner how many replicates there are.  Items in a plate are considered to be independent and identically distributed.  Bayes nets make it very easy to write out the posterior distribution of the parameters.  We simply follow the net from the bottom up, writing $ p(\alpha,\beta, \theta \vert y) \propto p(y\vert \theta)p(\theta \vert \alpha, \beta)p(\alpha,\beta) $, conditioning each node on it's parent nodes.

\begin{figure}[h!]

	\centering
	\begin{tikzpicture}
		\node[latent] (ab) {$\alpha,\beta$};
		\node[latent, below = of ab]  (theta) {$\theta$};
		\node[obs, below = of theta](y){$y$};
		
		\edge{ab}{theta};
		\edge{theta}{y};
		
		\plate{exp}{(theta)(y)}{$N$};
	\end{tikzpicture}
	\caption{Bayes net for hierarchical binomial model.  Note here that nodes with shading correspond to observed data, while nodes without shading are latent.}
	\label{bayesnet}
\end{figure}

Bayes nets can be used to do inference, and a literature of algorithms and methods exists for the purposes of doing so \cite{Bishop2006pattern,koller2009probabilistic}.  Here, and in subsequent writings, I will use them solely for economy of thought when describing models.


\subsubsection{Model Assessment}

Once a model is specified, and the posterior for the parameters obtained, the model fit, not only to the data but also to the practitioner's substantive knowledge, must be assessed.  

Since the result of a Bayesian analysis is a posterior distribution of the model parameters, it is easy to simulate data from the data generating process.  Let $ y $ be observed data, and $ \theta $ be a vector of (hyper)parameters for the model.  Denote $ \tilde{y} $ as replicated data from the data generating process, or as Gelman writes, ``data that we \textit{would} have seen tomorrow if the experiment that generated $ y $ today were replicated with the same model and the same value of $ \theta $ that produced the observed data" \cite[page~145]{gelman2013bayesian}.  Then the distribution of the replicated data conditioned on the observed data is 
%
\begin{equation}\label{PPD}
	p(\tilde{y} \vert y) = \int p(\tilde{y} \vert \theta) p(\theta \vert y) \, d\theta  \>.
\end{equation}
%
The distribution in \cref{PPD} is called the \textit{posterior predictive distribution}.  It stands to reason that if the model fits the data well, then observed data should look plausible under the posterior predictive distribution.  Simulated data sets are generated from \cref{PPD} and are compared to the observed data.  Any systematic differences between observed and simulated may point to areas in which the model can be improved.

Aspects of the observed data can be summarized into a \textit{test quantity} which is then compared against replicated data. This is usually some summary statistic of the observed data. Tail area probability computations can be used to quantify  the observed data's departure from the posterior predictive simulations.  If $ T(y) $ is a test quantity, then the tail area probability is $ P(T(\tilde{y}) > T(y) ) $.  This is similar to the Frequentist p-value.


\subsubsection{Markov Chain Monte Carlo \& Modern Methods}

The integrals in Bayesian statistics quickly become intractable when considering unusual models or relaxing assumptions for simple models.  Consequently, computational methods have been developed to aid in fitting models.  The result is the ability to sample from the posterior distribution without having to know the exact analytical form of the posterior density.

The suit of computational methods for sampling from the posterior are called \textit{Markov Chain Monte Carlo} (MCMC) methods.  These methods simulate Markov chains whose limiting distribution is the posterior distribution \cite{livingstone2016geometric}.  The most common MCMC methods for drawing  samples from the posterior are The Metropolis-Hastings Algorithm and Gibbs Sampling \cite{gelman2013bayesian,mcelreath2016statistical}.  Recently however, these algorithms have given way to more efficient algorithms, known as \textit{Hamiltonian Monte Carlo} (HMC) methods.  HMC is inspired by Hamiltonian mechanics. The posterior is idealized as a surface on which a particle of mass $ m $ rolls after being given a random position and momentum vector.  The geometry of the surface influences the movement of the particle, and thus influences the samples obtained.  HMC is a marvellously interesting method, spanning topics such as physics, numerical differential equations, and differential geometry.  The theory for HMC is quite dense, and more appropriate for a computer science department.  I refer interested readers to the following resources \cite{ gelman2013bayesian, livingstone2016geometric, mcelreath2016statistical,neal2011mcmc, hoffman2014no,betancourt2017conceptual}.

\subsubsection{Diagnostics for Hamiltonian Monte Carlo}

Provided the simulated Markov chain is geometrically ergodic, a law of large numbers and a central limit theorem exists for these Markov chains \cite{livingstone2016geometric, betancourt2017robust}, thus allowing users to be confident in inferences made from the samples obtained.  General conditions under which the chain is and is not geometrically ergodic exist \cite{livingstone2016geometric}, but verification of these properties for applied problems is not usually done analytically.  Instead, diagnostics are used to assess the reliability of inferences from the Markov chains.

The first diagnostic most applied Bayesians are introduced to is called the potential scale reduction factor, or Gelman-Rubin diagnostic, or $ \rhat$.    In MCMC and HMC, several chains are usually initialized and allowed to run for sufficiently long to as to (hopefully) arrive at their stationary distribution.  If geometric ergodicity holds, then all chains should arrive at the same stationary distribution, and thus be exploring the same space.  The Gelman-Rubin diagnostic  measures how well the chains are exploring the space by comparing the within chain variance to the between chain variance \cite{gelman2013bayesian}.  In practice, $ 1.1<\rhat $ indicates a problem with the chains, and inference should not be made from the samples drawn \cite{betancourt2017robust}.

Another diagnostic is the effective sample size, $ n_{eff} $.  The Markov chains generate correlated samples, not completely random samples, so using existing theories of convergence of functions of random variables are inappropriate.  Effective sample size is heuristic to measure how close the samples are to being independent.  Effective sample size is defined as
%
\[ n_{eff} = \dfrac{n}{1+ 2\displaystyle\sum_k \rho(k)} \>. \]
%
Here, $ \rho(k) $ is the lag-$k$ within chain correlation \cite{gelman2013bayesian,kass1998markov}.  If the chains are autocorrelated, then $n_{eff} <n$, and if the chains are completely independent $n_{eff} = n$. If chains are highly correlated, then $n_{eff} \ll n$, and inferences made from the samples should be avoided because of the bias the correlation would impart.