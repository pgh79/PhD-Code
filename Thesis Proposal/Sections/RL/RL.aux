\relax 
\citation{sutton2011reinforcement}
\citation{sutton2011reinforcement}
\citation{lizotte2017reinforcement}
\citation{sutton2011reinforcement}
\citation{lizotte2017reinforcement}
\citation{lizotte2017reinforcement}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Reinforcement Learning}{13}}
\@writefile{toc}{\contentsline {subsubsection}{Formalization of Sequential Decision Making}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Pictorial representation of the learning process. At the present state $ S_t $, the agent makes action $ A_t $. The action is rewarded with $ R_t $ and the agent finds itself in a new state $ S_{t+1} $. The process repeats and forms a trajectory of $ {S_t,A_t,R_t,S_{t+1}, A_{t+1}, R_{t+1}, S_{t+2}, \cdots  } $. \relax }}{13}}
\newlabel{RL_diagram}{{5}{13}}
\newlabel{RL_diagram@cref}{{[figure][5][]5}{[1][13][]13}}
\@writefile{toc}{\contentsline {subsubsection}{Policies}{13}}
\newlabel{key}{{11}{13}}
\newlabel{key@cref}{{[equation][11][]11}{[1][13][]13}}
\@writefile{toc}{\contentsline {subsubsection}{Goals for Learning}{14}}
\@setckpt{Sections/RL/RL}{
\setcounter{page}{15}
\setcounter{equation}{11}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{2}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{5}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{su@anzahl}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{ContinuedFloat}{0}
}
